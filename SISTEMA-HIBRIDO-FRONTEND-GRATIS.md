# SISTEMA H√çBRIDO FRONTEND - 100% GRATUITO

**Fecha:** 2025-10-28
**Objetivo:** APIs gratuitas para testing/feedback Sandra IA
**CEO:** Clayton Thomas
**Presupuesto:** 0 EUR (solo modelos gratuitos)

---

## üéØ ARQUITECTURA FINAL

### TIER SYSTEM OPTIMIZADO (GRATIS)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SANDRA IA FRONTEND - SISTEMA H√çBRIDO GRATIS   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   TIER 1 (80% queries)  ‚îÇ
        ‚îÇ   QWEN 2.5:7B (Ollama)  ‚îÇ
        ‚îÇ   ‚úÖ GRATIS - R√ÅPIDO    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   Si falla    ‚îÇ
              ‚ñº               ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   TIER 2 (15% queries)  ‚îÇ
        ‚îÇ DeepSeek Coder V2 (Oll) ‚îÇ
        ‚îÇ   ‚úÖ GRATIS - SMART     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   Si falla    ‚îÇ
              ‚ñº               ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   TIER 3 (4% queries)   ‚îÇ
        ‚îÇ   Llama 3.1 (Ollama)    ‚îÇ
        ‚îÇ   ‚úÖ GRATIS - ESTABLE   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  Emergencia   ‚îÇ
              ‚ñº               ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   TIER 4 (1% queries)   ‚îÇ
        ‚îÇ   GROQ Mixtral (API)    ‚îÇ
        ‚îÇ   ‚ö†Ô∏è GRATIS LIMITADO    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ MODELOS A INSTALAR (OLLAMA)

### 1. QWEN 2.5:7B (Primario - Conversacional)
```bash
ollama pull qwen2.5:7b
```

**Caracter√≠sticas:**
- Tama√±o: 4.7 GB
- RAM necesaria: 8 GB
- Velocidad: Muy r√°pida
- Espa√±ol: Excelente
- Conversacional: Excelente
- **Uso:** 80% de consultas de Sandra

**Por qu√© es perfecto para Sandra:**
- Entrenado espec√≠ficamente para conversaci√≥n multiling√ºe
- Excelente comprensi√≥n de contexto
- Respuestas naturales y emp√°ticas
- R√°pido para experiencia en tiempo real

---

### 2. DEEPSEEK CODER V2:16B (Secundario - Razonamiento)
```bash
ollama pull deepseek-coder-v2:16b
```

**Caracter√≠sticas:**
- Tama√±o: 8.9 GB
- RAM necesaria: 16 GB
- Velocidad: R√°pida
- Espa√±ol: Muy buena
- Razonamiento: Excelente
- **Uso:** 15% de consultas complejas

**Por qu√© como fallback:**
- Mejor razonamiento que Qwen
- Contexto largo (128K tokens)
- Excelente para consultas complejas
- Backup robusto

---

### 3. LLAMA 3.1:8B (Terciario - Estabilidad)
```bash
ollama pull llama3.1:8b
```

**Caracter√≠sticas:**
- Tama√±o: 4.7 GB
- RAM necesaria: 8 GB
- Velocidad: Media
- Espa√±ol: Buena
- Estabilidad: Excelente
- **Uso:** 4% fallback final

**Por qu√© como √∫ltimo recurso:**
- Muy estable y probado
- Funciona siempre
- Backup confiable

---

## üöÄ INSTALACI√ìN R√ÅPIDA

### Paso 1: Verificar Ollama instalado
```bash
ollama --version
```

Si no est√° instalado:
```bash
# Windows
winget install Ollama.Ollama

# O descargar desde https://ollama.com
```

---

### Paso 2: Descargar los 3 modelos
```bash
# Modelo primario (OBLIGATORIO)
ollama pull qwen2.5:7b

# Modelo secundario (RECOMENDADO)
ollama pull deepseek-coder-v2:16b

# Modelo terciario (OPCIONAL)
ollama pull llama3.1:8b
```

**Tiempo estimado:** 15-30 minutos (dependiendo de conexi√≥n)
**Espacio disco:** ~18 GB total

---

### Paso 3: Probar los modelos
```bash
# Probar Qwen
ollama run qwen2.5:7b "Hola, soy Sandra. ¬øEn qu√© puedo ayudarte hoy?"

# Probar DeepSeek
ollama run deepseek-coder-v2:16b "Expl√≠came brevemente qu√© servicios ofrece GuestsValencia"

# Probar Llama
ollama run llama3.1:8b "Hola, cu√©ntame sobre ti"
```

---

## üíª C√ìDIGO DE INTEGRACI√ìN

### Backend: Nuevo endpoint `/api/chat-local`

```javascript
// netlify/functions/chat-local/index.js
const fetch = require('node-fetch');

// Llamar a Ollama local (Qwen primario)
async function callQwen(messages) {
  const systemPrompt = {
    role: 'system',
    content: `Eres Sandra, asistente virtual de GuestsValencia.

PERSONALIDAD:
- C√°lida, emp√°tica y profesional
- Hablas espa√±ol de Espa√±a (no latino)
- Respondes de forma natural y conversacional
- Usas un tono amigable pero profesional

CONTEXTO:
- GuestsValencia: Gesti√≥n de alquileres vacacionales en Valencia
- Servicios: Reservas, atenci√≥n hu√©spedes, gesti√≥n propiedades
- Tu misi√≥n: Ayudar a hu√©spedes y propietarios

ESTILO:
- Respuestas breves (2-3 frases m√°ximo)
- Directa y clara
- Emp√°tica con las necesidades del usuario
- Siempre dispuesta a ayudar

IMPORTANTE:
- Si no sabes algo, lo admites honestamente
- Ofreces alternativas cuando no puedes ayudar directamente
- Mantienes la conversaci√≥n fluida y natural`
  };

  try {
    const response = await fetch('http://localhost:11434/api/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: 'qwen2.5:7b',
        messages: [systemPrompt, ...messages],
        stream: false,
        options: {
          temperature: 0.7,
          top_p: 0.9,
          top_k: 40
        }
      })
    });

    if (!response.ok) throw new Error(`Qwen failed: ${response.status}`);

    const data = await response.json();
    return {
      text: data.message.content,
      provider: 'Qwen 2.5 (Local)'
    };
  } catch (error) {
    throw new Error(`Qwen error: ${error.message}`);
  }
}

// Fallback a DeepSeek
async function callDeepSeek(messages) {
  const response = await fetch('http://localhost:11434/api/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model: 'deepseek-coder-v2:16b',
      messages: messages,
      stream: false,
      options: {
        temperature: 0.6,
        top_p: 0.85
      }
    })
  });

  if (!response.ok) throw new Error(`DeepSeek failed: ${response.status}`);

  const data = await response.json();
  return {
    text: data.message.content,
    provider: 'DeepSeek V2 (Local)'
  };
}

// Fallback a Llama 3.1
async function callLlama(messages) {
  const response = await fetch('http://localhost:11434/api/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model: 'llama3.1:8b',
      messages: messages,
      stream: false
    })
  });

  if (!response.ok) throw new Error(`Llama failed: ${response.status}`);

  const data = await response.json();
  return {
    text: data.message.content,
    provider: 'Llama 3.1 (Local)'
  };
}

// Fallback a GROQ (gratis limitado)
async function callGROQ(messages) {
  const apiKey = process.env.GROQ_API_KEY;
  if (!apiKey) throw new Error('GROQ_API_KEY not configured');

  const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: 'mixtral-8x7b-32768',
      messages: messages,
      temperature: 0.7,
      max_tokens: 500
    })
  });

  if (!response.ok) throw new Error(`GROQ failed: ${response.status}`);

  const data = await response.json();
  return {
    text: data.choices[0].message.content,
    provider: 'GROQ Mixtral (API)'
  };
}

// Handler principal con cascada
exports.handler = async (event) => {
  const headers = {
    'Content-Type': 'application/json',
    'Access-Control-Allow-Origin': '*',
    'Access-Control-Allow-Methods': 'POST, OPTIONS',
    'Access-Control-Allow-Headers': 'Content-Type'
  };

  if (event.httpMethod === 'OPTIONS') {
    return { statusCode: 200, headers, body: '' };
  }

  try {
    const { messages = [] } = JSON.parse(event.body || '{}');

    if (!messages || messages.length === 0) {
      return {
        statusCode: 400,
        headers,
        body: JSON.stringify({ error: 'No messages provided' })
      };
    }

    let result;

    // TIER 1: Qwen 2.5 (primario - gratis local)
    try {
      console.log('ü§ñ Intentando Qwen 2.5 (Local Tier 1)...');
      result = await callQwen(messages);
      console.log('‚úÖ Qwen 2.5 respondi√≥ exitosamente');
      return {
        statusCode: 200,
        headers,
        body: JSON.stringify({
          text: result.text,
          provider: result.provider,
          tier: 1,
          cost: 'FREE'
        })
      };
    } catch (qwenError) {
      console.warn('‚ö†Ô∏è Qwen fall√≥:', qwenError.message);

      // TIER 2: DeepSeek (secundario - gratis local)
      try {
        console.log('ü§ñ Intentando DeepSeek Coder V2 (Local Tier 2)...');
        result = await callDeepSeek(messages);
        console.log('‚úÖ DeepSeek respondi√≥ exitosamente');
        return {
          statusCode: 200,
          headers,
          body: JSON.stringify({
            text: result.text,
            provider: result.provider,
            tier: 2,
            cost: 'FREE'
          })
        };
      } catch (deepseekError) {
        console.warn('‚ö†Ô∏è DeepSeek fall√≥:', deepseekError.message);

        // TIER 3: Llama 3.1 (terciario - gratis local)
        try {
          console.log('ü§ñ Intentando Llama 3.1 (Local Tier 3)...');
          result = await callLlama(messages);
          console.log('‚úÖ Llama 3.1 respondi√≥ exitosamente');
          return {
            statusCode: 200,
            headers,
            body: JSON.stringify({
              text: result.text,
              provider: result.provider,
              tier: 3,
              cost: 'FREE'
            })
          };
        } catch (llamaError) {
          console.warn('‚ö†Ô∏è Llama fall√≥:', llamaError.message);

          // TIER 4: GROQ (emergencia - gratis limitado API)
          try {
            console.log('ü§ñ Intentando GROQ Mixtral (API Tier 4)...');
            result = await callGROQ(messages);
            console.log('‚úÖ GROQ respondi√≥ exitosamente');
            return {
              statusCode: 200,
              headers,
              body: JSON.stringify({
                text: result.text,
                provider: result.provider,
                tier: 4,
                cost: 'FREE (limited)'
              })
            };
          } catch (groqError) {
            console.error('‚ùå Todos los modelos fallaron');
            throw new Error('All free models failed');
          }
        }
      }
    }
  } catch (error) {
    console.error('Handler error:', error);
    return {
      statusCode: 500,
      headers,
      body: JSON.stringify({
        error: error.message || 'Internal server error',
        text: 'Error en la IA. Por favor reint√©ntalo.'
      })
    };
  }
};
```

---

## üé® PROMPT ENGINEERING OPTIMIZADO

### Sistema de Prompts por Contexto

```javascript
const SANDRA_PROMPTS = {
  // Prompt base para todos los modelos
  base: `Eres Sandra, asistente virtual de GuestsValencia.

PERSONALIDAD:
- C√°lida, emp√°tica y profesional
- Hablas espa√±ol de Espa√±a natural
- Respondes de forma conversacional
- Tono amigable pero profesional

SERVICIOS GUESTSVALENCIA:
- Gesti√≥n alquileres vacacionales Valencia
- Atenci√≥n hu√©spedes 24/7
- Gesti√≥n propiedades para propietarios
- Reservas y coordinaci√≥n llegadas/salidas

TU MISI√ìN:
- Ayudar a hu√©spedes con dudas sobre alojamiento
- Asistir propietarios en gesti√≥n propiedades
- Proporcionar informaci√≥n sobre servicios
- Mantener conversaci√≥n natural y √∫til`,

  // Prompt mejorado para Qwen (conversacional)
  qwen: `Eres Sandra, la voz de GuestsValencia.

üéØ TU ESENCIA:
- Asistente IA c√°lida y profesional
- Experta en hospitalidad valenciana
- Hablas espa√±ol natural de Espa√±a
- Empatizas con hu√©spedes y propietarios

üíº GUESTSVALENCIA:
- Gesti√≥n premium de alquileres vacacionales
- Ubicaci√≥n: Valencia, Espa√±a
- Servicios 24/7 para m√°xima satisfacci√≥n
- Tecnolog√≠a + toque humano

üó£Ô∏è TU ESTILO:
- Respuestas breves (2-3 frases)
- Natural y conversacional
- Directa pero amable
- Enfocada en soluciones

üí° RECUERDA:
- Si no sabes algo, lo admites honestamente
- Ofreces alternativas cuando no puedes ayudar
- Mantienes la conversaci√≥n fluida
- Priorizas la experiencia del usuario`,

  // Prompt para DeepSeek (consultas complejas)
  deepseek: `Contexto: Eres Sandra, IA de GuestsValencia para consultas complejas.

Caracter√≠sticas:
- An√°lisis detallado cuando necesario
- Razonamiento paso a paso
- Informaci√≥n precisa y verificable
- Espa√±ol profesional

Objetivo: Resolver consultas espec√≠ficas con precisi√≥n manteniendo el tono de Sandra.`,

  // Prompt para Llama (fallback)
  llama: `System: You are Sandra, GuestsValencia's AI assistant.

Key points:
- Friendly and professional
- Speak natural Spanish (Spain)
- Brief responses (2-3 sentences)
- Focus on helping users

Response in Spanish, naturally and warmly.`
};
```

---

## üìä M√âTRICAS Y MONITOREO

### Dashboard de Uso (para optimizar)

```javascript
const METRICS = {
  usage: {
    tier1_qwen: 0,      // Contador llamadas Qwen
    tier2_deepseek: 0,  // Contador llamadas DeepSeek
    tier3_llama: 0,     // Contador llamadas Llama
    tier4_groq: 0       // Contador llamadas GROQ (‚ö†Ô∏è costo)
  },

  latency: {
    qwen: [],
    deepseek: [],
    llama: [],
    groq: []
  },

  quality: {
    user_satisfaction: [],  // Rating 1-5
    response_length: [],    // Tokens de respuesta
    fallback_rate: 0        // % que llegaron a Tier 2+
  }
};

function trackMetric(tier, model, latency, tokens) {
  METRICS.usage[`tier${tier}_${model}`]++;
  METRICS.latency[model].push(latency);

  // Log para an√°lisis
  console.log(`üìä M√©tricas: Tier ${tier} | ${model} | ${latency}ms | ${tokens} tokens`);
}
```

---

## üéØ VENTAJAS DEL SISTEMA

### ECON√ìMICAS:
- ‚úÖ **100% GRATIS** para 99% de consultas
- ‚úÖ Solo GROQ en emergencias (gratis limitado)
- ‚úÖ Cero costo mensual recurrente
- ‚úÖ Escalable sin incremento de costo

### T√âCNICAS:
- ‚úÖ Latencia baja (Ollama local r√°pido)
- ‚úÖ Sin l√≠mites de rate (es local)
- ‚úÖ Privacidad (datos no salen del servidor)
- ‚úÖ Redundancia (4 tiers de fallback)

### OPERATIVAS:
- ‚úÖ Ideal para testing masivo
- ‚úÖ Perfecto para crear feedback
- ‚úÖ Sin presi√≥n de costos
- ‚úÖ Puedes hacer miles de pruebas gratis

---

## üöÄ PR√ìXIMOS PASOS

### FASE 1: Instalaci√≥n (15-30 min)
1. Instalar Ollama en servidor
2. Descargar Qwen 2.5:7b (obligatorio)
3. Descargar DeepSeek V2:16b (recomendado)
4. Descargar Llama 3.1:8b (opcional)
5. Probar cada modelo individualmente

### FASE 2: Integraci√≥n (30 min)
6. Crear `/api/chat-local` en Netlify Functions
7. Configurar cascada de fallback
8. Implementar prompt engineering optimizado
9. A√±adir m√©tricas de tracking

### FASE 3: Frontend (15 min)
10. Actualizar `sandra-mobile.js`
11. Cambiar endpoint de `/api/chat` a `/api/chat-local`
12. A√±adir indicador de modelo usado
13. Deploy y testing

### FASE 4: Optimizaci√≥n (continua)
14. Analizar m√©tricas de uso
15. Ajustar prompts seg√∫n feedback
16. Optimizar para que 95%+ use Qwen (Tier 1)
17. Minimizar uso de GROQ (Tier 4)

---

## üí° RECOMENDACIONES FINALES

### Para Testing con Sandrita:
- Usa **Qwen 2.5** - Excelente para conversaci√≥n natural
- Ajusta prompts para ense√±anza de ingl√©s
- Monitorea qu√© modelo responde mejor

### Para Feedback Falso (Pruebas):
- Haz miles de consultas sin preocuparte por costo
- Prueba edge cases y errores
- Perfecciona el sistema tranquilamente

### Cuando Perfecciones Sistema:
- Cambia endpoint a GPT-4o para producci√≥n
- Mant√©n Ollama como backup gratuito
- Usa m√©tricas para decidir cu√°ndo cambiar

---

## üìù NOTAS IMPORTANTES

### RAM Necesaria:
- **M√≠nimo:** 8 GB (para Qwen o Llama)
- **Recomendado:** 16 GB (para DeepSeek tambi√©n)
- **√ìptimo:** 32 GB (todos los modelos + OS)

### Espacio Disco:
- Qwen 2.5:7b ‚Üí 4.7 GB
- DeepSeek V2:16b ‚Üí 8.9 GB
- Llama 3.1:8b ‚Üí 4.7 GB
- **Total:** ~18 GB

### Servidor:
- Ollama se ejecuta en `localhost:11434`
- Netlify Functions llama a Ollama local
- Aseg√∫rate de que Ollama est√© ejecut√°ndose

---

**Sistema dise√±ado para:** Testing masivo sin costo, feedback, perfeccionamiento
**Migraci√≥n a producci√≥n:** Cambiar endpoint cuando sistema est√© perfecto
**Costo operativo:** 0 EUR (100% gratuito local)

---

üöÄ **LISTO PARA IMPLEMENTAR** - Solo necesitas confirmar y empezamos instalaci√≥n.
